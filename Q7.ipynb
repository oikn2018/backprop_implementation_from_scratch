{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSZHns9MwPtRZowN2LbJS9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oikn2018/CS6910_assignment_1/blob/main/Q7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing required packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.datasets import fashion_mnist\n",
        "# import wandb\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
        "\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "saXMxdRYT8vF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fashion_mnist dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
        "\n",
        "print(f\"Training data shape: {x_train.shape}, Training label shape: {y_train.shape}\")\n",
        "print(f\"Validation data shape: {x_val.shape}, Validation label shape: {y_val.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gUJXUEZT9q9",
        "outputId": "cd5c9407-d566-43c9-d81d-848c3fc1197f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Training data shape: (54000, 28, 28), Training label shape: (54000,)\n",
            "Validation data shape: (6000, 28, 28), Validation label shape: (6000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the input data for training, validation, and testing sets\n",
        "X_train = np.reshape(x_train, (x_train.shape[0], -1)).T\n",
        "X_val = np.reshape(x_val, (x_val.shape[0], -1)).T\n",
        "X_test = np.reshape(x_test, (x_test.shape[0], -1)).T\n",
        "\n",
        "# Normalize the input data to have values between 0 and 1\n",
        "X_train = X_train / 255.\n",
        "X_val = X_val / 255.\n",
        "X_test = X_test / 255.\n",
        "\n",
        "# Convert the target labels into one-hot encoded vectors\n",
        "Y_train = np.eye(np.max(y_train) + 1)[y_train].T\n",
        "Y_val = np.eye(np.max(y_val) + 1)[y_val].T\n",
        "Y_test = np.eye(np.max(y_test) + 1)[y_test].T\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}, Training label shape: {Y_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}, Validation label shape: {Y_val.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}, Testing label shape: {Y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfkegBAwT9an",
        "outputId": "ec631fc1-369c-49ac-c1eb-df1056b8e050"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (784, 54000), Training label shape: (10, 54000)\n",
            "Validation data shape: (784, 6000), Validation label shape: (10, 6000)\n",
            "Testing data shape: (784, 10000), Testing label shape: (10, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNN:\n",
        "    def __init__(self,config=None,epochs=20,hidden_layers=[512, 512, 512, 512, 512],weight_decay=0,learning_rate=0.005,optimizer='nadam',batch_size=64,weight_initialization='xavier',activations='sigmoid',loss_function='cross-entropy',output_function='softmax',gamma=0.9,beta=0.9,beta1=0.9,beta2=0.999,eps=1e-8):\n",
        "        \n",
        "        # self.run_name = \"lr_{}_ac_{}_in_{}_op_{}_bs_{}_ep_{}_nn_{}_nh_{}\".format(self.learning_rate, self.activations, self.weight_initialization, self.optimizer, self.batch_size, self.epochs, self.hidden_layers, len(self.hidden_layers))\n",
        "\n",
        "        # Constructor that initializes the neural network\n",
        "        if config is not None:\n",
        "            # If a config dictionary is passed, use its values to initialize the parameters\n",
        "            self.epochs = config[\"epochs\"]\n",
        "            self.learning_rate = config[\"learning_rate\"]\n",
        "            self.weight_decay = config[\"weight_decay\"]\n",
        "            self.loss_function = config[\"loss_function\"]\n",
        "            self.optimizer = config[\"optimizer\"]\n",
        "            self.batch_size = config[\"batch_size\"]\n",
        "            self.weight_initialization = config[\"weight_initialization\"]\n",
        "            self.activations = config[\"activations\"]\n",
        "            self.hidden_layers = [config[\"hidden_layers_size\"] for x in range(config[\"no_hidden_layers\"])]\n",
        "        else:\n",
        "            # If no config dictionary is passed, use the default values to initialize the parameters\n",
        "            self.epochs = epochs\n",
        "            self.learning_rate = learning_rate\n",
        "            self.weight_decay = weight_decay\n",
        "            self.loss_function = loss_function\n",
        "            self.optimizer = optimizer\n",
        "            self.batch_size = batch_size\n",
        "            self.weight_initialization = weight_initialization\n",
        "            self.activations = activations\n",
        "            self.hidden_layers = hidden_layers\n",
        "\n",
        "        # Set the remaining parameters for the neural network\n",
        "        # self.loss_function = loss_function\n",
        "        self.output_function = output_function\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "\n",
        "        self.run_name = \"loss_{}_lr_{}_ac_{}_in_{}_op_{}_bs_{}_ep_{}_nn_{}\".format(self.loss_function, self.learning_rate, self.activations, self.weight_initialization, self.optimizer, self.batch_size, self.epochs, self.hidden_layers)\n",
        "        # Initialize the neural network\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        # Set the number of neurons in each layer of the neural network\n",
        "        layers = self.hidden_layers + [Y_train.shape[0]]\n",
        "\n",
        "        # Initialize the weights and biases for each layer of the neural network\n",
        "        self.theta = self.initialize_parameters(X_train.shape[0],layers,self.weight_initialization)\n",
        "\n",
        "        # Calculate the regularization parameter\n",
        "        self.lambd = self.weight_decay/self.learning_rate\n",
        "\n",
        "        # Set the number of layers in the neural network\n",
        "        self.L = len(layers)\n",
        "\n",
        "\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return (1.0 / (1.0 + np.exp(-x)) )\n",
        "    \n",
        "    def tanh(self,x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def relu(self,x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def d_sigmoid(self,x):\n",
        "        return (self.sigmoid(x)*(1.0 - self.sigmoid(x)))\n",
        "    \n",
        "    def d_tanh(self,x):\n",
        "        return 1.0 - (self.tanh(x)**2)\n",
        "\n",
        "    def d_relu(self,x):\n",
        "        return np.greater(x, 0).astype(int)\n",
        "\n",
        "    def activation(self,x,activation_function='sigmoid'):\n",
        "        if activation_function == 'sigmoid':\n",
        "              return self.sigmoid(x)\n",
        "        elif activation_function == 'tanh':\n",
        "              return self.tanh(x)\n",
        "        elif activation_function == 'relu':\n",
        "              return self.relu(x)\n",
        "\n",
        "    def d_activation(self,x,activation_function='sigmoid'):\n",
        "        if activation_function == 'sigmoid':\n",
        "              return self.d_sigmoid(x)\n",
        "        elif activation_function == 'tanh':\n",
        "              return self.d_tanh(x)\n",
        "        elif activation_function=='relu':\n",
        "              return self.d_relu(x)\n",
        "\n",
        "\n",
        "    def softmax(self,x):\n",
        "        e = np.exp(x)\n",
        "        return e / np.sum(e,axis=0)\n",
        "\n",
        "    def output(self, x, output_function='softmax'):\n",
        "        if output_function == 'softmax':\n",
        "            return self.softmax(x)\n",
        "\n",
        "    def cross_entropy_error(self, Y, inputs):\n",
        "        Y_hat = inputs[1][-1]\n",
        "        return -1 * np.sum(Y * (np.log(Y_hat)))\n",
        "    \n",
        "    def squared_error(self, Y, inputs):\n",
        "        Y_hat = inputs[1][-1]\n",
        "        return (1 / 2) * np.sum((Y_hat - Y) ** 2)\n",
        "\n",
        "    def error(self, Y, inputs, loss_function='cross-entropy'):\n",
        "        if loss_function == 'cross-entropy':\n",
        "            return self.cross_entropy_error(Y, inputs)\n",
        "        elif loss_function == 'squared-error':\n",
        "            return self.squared_error(Y, inputs) \n",
        "\n",
        "    def squared_error_val(self, Y, inputs):\n",
        "        Y_hat = inputs[1][-1]\n",
        "        W, B = self.theta\n",
        "        m = Y.shape[1]\n",
        "        return (1 / (2 * m)) * np.sum((Y_hat - Y) ** 2) + (self.lambd / (2 * m)) * (self.frobenius(W ** 2) + self.frobenius(B ** 2))\n",
        "\n",
        "    def cross_entropy_error_val(self, Y, inputs):\n",
        "        W, B = self.theta\n",
        "        Y_hat = inputs[1][-1]\n",
        "        m = Y.shape[1]\n",
        "        return (-1/m) * np.sum(Y * (np.log(Y_hat))) + (self.lambd/(2*m)) * (self.frobenius(W ** 2) + self.frobenius(B ** 2))\n",
        "\n",
        "    def val_error(self, Y, inputs, loss_function='cross-entropy'):\n",
        "        if loss_function == 'cross-entropy':\n",
        "            return self.cross_entropy_error_val(Y, inputs)\n",
        "        if loss_function == 'squared-error':\n",
        "            return self.squared_error_val(Y, inputs)\n",
        "\n",
        "#     def frobenius(self, M):\n",
        "#         return np.sqrt(np.sum(np.square(M)))\n",
        "\n",
        "    def initialize_params_random(self, n, layers):\n",
        "        L = len(layers)\n",
        "        biases = [np.float128(np.zeros((layers[i], 1))) for i in range(L)]\n",
        "        weights = [np.float128(np.random.randn(layers[i], n) if i == 0 else np.random.randn(layers[i], layers[i - 1])) for i in range(L)]\n",
        "        return (np.array(weights), np.array(biases))\n",
        "\n",
        "    def initialize_params_xavier(self, n, layers):\n",
        "        \n",
        "        L=len(layers)\n",
        "        biases = []\n",
        "        weights = []\n",
        "        for i in range(L):\n",
        "            bias=np.float128(np.zeros((layers[i], 1)))\n",
        "            if i==0:\n",
        "                 weight=np.float128(np.random.randn(layers[i],n))\n",
        "            else:\n",
        "                 weight = np.float128(np.random.randn(layers[i],layers[i - 1]) * np.sqrt(1 / layers[i - 1]))\n",
        "            biases.append(bias)\n",
        "            weights.append(weight)\n",
        "\n",
        "        return (np.array(weights),np.array(biases))\n",
        "#         biases = [np.float128(np.zeros((layer, 1))) for layer in layers]\n",
        "#         weights = [np.float128(np.random.randn(layers[i], n if i == 0 else layers[i - 1]) * np.sqrt(1 / (n if i == 0 else layers[i - 1]))) for i in range(len(layers))]\n",
        "#         return np.array(weights), np.array(biases)\n",
        "\n",
        "    def initialize_parameters(self,n,layers,param_init_type):\n",
        "        if param_init_type == 'random':\n",
        "            return self.initialize_params_random(n,layers)\n",
        "        elif param_init_type == 'xavier':\n",
        "            return self.initialize_params_xavier(n,layers)\n",
        "\n",
        "    def frobenius(self,X):\n",
        "        s=0\n",
        "        for x in X:\n",
        "          s += np.sum(x)\n",
        "        return s\n",
        "#     def frobenius(self, X):\n",
        "#         return np.linalg.norm(X, ord='fro')\n",
        "\n",
        "    def feedforward(self,X,theta,L):\n",
        "        H = X\n",
        "        weights ,biases = theta\n",
        "        activations = []\n",
        "        pre_activations = []\n",
        "        for k in range(L-1):\n",
        "              A = biases[k] + (weights[k] @ H)\n",
        "              H = self.activation(A,self.activations)\n",
        "              pre_activations.append(A)\n",
        "              activations.append(H)\n",
        "        \n",
        "        AL = biases[L-1] + (weights[L-1] @ H)\n",
        "        Y_hat = self.output(AL,self.output_function)\n",
        "        pre_activations.append(AL)\n",
        "        activations.append(Y_hat)\n",
        "        return (np.array(pre_activations),np.array(activations))\n",
        "\n",
        "    def backprop(self,X,Y,inputs,theta,batch_size,L):\n",
        "        # Initialize empty lists for storing gradients\n",
        "        d_biases, d_weights = [], []\n",
        "        d_biases2 = []\n",
        "        d_weights2 = []\n",
        "        # Extract pre-activations and activations from the inputs\n",
        "        pre_activations, activations = inputs\n",
        "        # Get the predicted output\n",
        "        Y_hat = activations[-1]\n",
        "#         # Retrieve the weights and biases from the current model parameters\n",
        "#         weights, biases = theta\n",
        "\n",
        "        if self.loss_function == 'squared-error':\n",
        "          d_AL = Y_hat*(Y_hat - Y)*(1 - Y_hat)\n",
        "        elif self.loss_function == 'cross-entropy':\n",
        "          d_AL = Y_hat - Y\n",
        "        # Loop over the layers in reverse order to calculate the gradients\n",
        "        for k in range(L-1, -1, -1):\n",
        "            # Calculate the gradients for the weights and biases\n",
        "            d_W = (1/batch_size)*(d_AL @ activations[k-1].T) if k > 0 else (1/batch_size)*(d_AL @ X.T)\n",
        "            d_W2 = (1 / batch_size) * (d_AL ** 2 @ (activations[k-1].T) ** 2) if k>0 else (1 / batch_size) * (d_AL ** 2 @ (X.T) ** 2)\n",
        "            d_B = (1/batch_size)*np.sum(d_AL, axis=1, keepdims=True)\n",
        "            d_B2 = (1 / batch_size) * np.sum(d_AL ** 2, axis=1, keepdims=True)\n",
        "\n",
        "            # Calculate the derivative of the activation function and backpropagate the error to the previous layer\n",
        "            if k > 0:\n",
        "                d_AL = (theta[0][k].T @ d_AL) * self.d_activation(pre_activations[k-1], self.activations)\n",
        "            # Add the gradients to the lists\n",
        "            d_weights.insert(0, d_W)\n",
        "            d_biases.insert(0, d_B)\n",
        "            d_weights2.insert(0, d_W2)\n",
        "            d_biases2.insert(0, d_B2)\n",
        "        d_theta = (np.array(d_weights),np.array(d_biases))\n",
        "        d_theta2 = (np.array(d_weights2), np.array(d_biases2))\n",
        "        \n",
        "        return (d_theta, d_theta2)\n",
        "\n",
        "    # Function to update weights and biases based on the calculated gradients and learning rate\n",
        "    def update_params(self, theta, d_theta, learning_rate):\n",
        "        weights, biases = theta\n",
        "        d_weights, d_biases = d_theta\n",
        "        updated_weights = (1 - self.weight_decay)*weights - learning_rate*d_weights\n",
        "        updated_biases = (1 - self.weight_decay)*biases - learning_rate*d_biases\n",
        "        return updated_weights, updated_biases\n",
        "\n",
        "    # Function to perform mini-batch gradient descent on the given data\n",
        "    def sgd(self, X, Y, theta, learning_rate, batch_size, L):\n",
        "        m = X.shape[1]\n",
        "        total_error = 0\n",
        "        # loop over mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            start = i\n",
        "            stop = i + batch_size\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            W, B = theta\n",
        "            # compute L2 regularization term\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            # compute error\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization\n",
        "            # compute gradients\n",
        "            d_theta, _ = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) \n",
        "            # update weights and biases                                                                    \n",
        "            theta = self.update_params(theta, d_theta, learning_rate)\n",
        "        # handle the last mini-batch if it is not a multiple of batch_size\n",
        "        if m % batch_size != 0:\n",
        "            start = m - m % batch_size\n",
        "            stop = m\n",
        "#             start = i*batch_size\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            d_theta, _ = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, m % batch_size, L) # compute gradients\n",
        "            theta = self.update_params(theta, d_theta, learning_rate) # update weights and biases\n",
        "            W, B = theta\n",
        "\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization \n",
        "            \n",
        "        # Calculate the average error\n",
        "        avg_err = total_error/m\n",
        "        # Return the updated theta and average error\n",
        "        return (theta, avg_err)\n",
        "    \n",
        "    def update_params_momentum(self, theta, d_theta, learning_rate, gamma, prev_weight, prev_bias):\n",
        "        weights, biases = theta\n",
        "        d_weights,d_biases = d_theta\n",
        "        \n",
        "        # Calculate the velocity for weights and biases\n",
        "        v_weight = gamma * prev_weight + learning_rate * d_weights\n",
        "        v_bias = gamma * prev_bias + learning_rate * d_biases\n",
        "\n",
        "        # Apply weight decay to the weights\n",
        "        decay = (1 - self.weight_decay)\n",
        "\n",
        "        # Update weights and biases using the velocity and decayed weights\n",
        "        updated_weights = decay*weights - v_weight\n",
        "        updated_biases = decay*biases - v_bias\n",
        "\n",
        "        return (updated_weights, updated_biases, v_weight, v_bias)\n",
        "\n",
        "    def gd_momentum(self, X, Y, theta, learning_rate, batch_size, gamma, L):\n",
        "        m = X.shape[1] # number of training examples\n",
        "        prev_weights = 0 # initialize previous weights to zero\n",
        "        prev_biases = 0 # initialize previous biases to zero\n",
        "        total_error = 0 # initialize total error to zero\n",
        "\n",
        "        # loop over mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            start = i\n",
        "            stop = i + batch_size\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            W, B = theta\n",
        "            # compute L2 regularization term\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            # compute error\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization\n",
        "            # compute gradients\n",
        "            d_theta, _ = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) \n",
        "            # update weights and biases using momentum                                                                     \n",
        "            weights, biases, prev_weights, prev_biases = self.update_params_momentum(theta, d_theta, learning_rate, gamma, prev_weights, prev_biases)\n",
        "            theta = weights, biases\n",
        "        # handle the last mini-batch if it is not a multiple of batch_size\n",
        "        if m % batch_size != 0:\n",
        "            start = m - m % batch_size\n",
        "            stop = m\n",
        "#             start = i*batch_size\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            d_theta = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, m % batch_size, L) # compute gradients\n",
        "        \n",
        "            weights, biases, prev_weights, prev_biases = self.update_params_momentum(theta, d_theta, learning_rate, gamma, prev_weights, prev_biases) # update weights and biases using momentum\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(weights**2) + self.frobenius(biases**2) )\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization \n",
        "            \n",
        "            \n",
        "\n",
        "            theta = weights, biases\n",
        "        \n",
        "        total_error /= m # average total error across all mini-batches\n",
        "        return (theta, total_error) # return updated weights and biases and the total error\n",
        "\n",
        "    def update_params_nesterov(self, theta, d_theta, learning_rate, gamma, prev_weight, prev_bias):\n",
        "        weights, biases = theta\n",
        "        d_weights,d_biases = d_theta\n",
        "        \n",
        "        # Calculate the velocity for weights and biases\n",
        "        v_weight = gamma * prev_weight + learning_rate * d_weights\n",
        "        v_bias = gamma * prev_bias + learning_rate * d_biases\n",
        "\n",
        "        # Apply weight decay to the weights\n",
        "        decay = (1 - self.weight_decay)\n",
        "\n",
        "        # Update weights and biases using the velocity and decayed weights\n",
        "        updated_weights = decay*weights - v_weight\n",
        "        updated_biases = decay*biases - v_bias\n",
        "\n",
        "        return (updated_weights, updated_biases, v_weight, v_bias)\n",
        "\n",
        "    def gd_nesterov(self, X, Y, theta, learning_rate, batch_size, gamma, L):\n",
        "        m = X.shape[1] # number of training examples\n",
        "        prev_weights = 0 # initialize previous weights to zero\n",
        "        prev_biases = 0 # initialize previous biases to zero\n",
        "        total_error = 0 # initialize total error to zero\n",
        "\n",
        "        weights, biases = theta\n",
        "        # loop over mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            start = i\n",
        "            stop = i + batch_size\n",
        "            # compute output of the network\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) \n",
        "            W, B = theta\n",
        "            # compute L2 regularization term\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            # compute error\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            v_weight=gamma*prev_weights\n",
        "            v_biases=gamma*prev_biases\n",
        "            theta2=weights-v_weight,biases-v_biases\n",
        "            d_theta, _ = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta2, batch_size, L) \n",
        "            \n",
        "            # update weights and biases using momentum                                                                     \n",
        "            weights, biases, prev_weights, prev_biases = self.update_params_nesterov(theta, d_theta, learning_rate, gamma, prev_weights, prev_biases)\n",
        "            theta = weights, biases\n",
        "        # handle the last mini-batch if it is not a multiple of batch_size\n",
        "        if m % batch_size != 0:\n",
        "            start = m - m % batch_size\n",
        "            stop = m\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            v_weight=gamma*prev_weights\n",
        "            v_biases=gamma*prev_biases\n",
        "            theta2=weights-v_weight,biases-v_biases\n",
        "            d_theta, _= self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta2, m % batch_size, L) # compute gradients\n",
        "        \n",
        "            weights, biases, prev_weights, prev_biases = self.update_params_nesterov(theta, d_theta, learning_rate, gamma, prev_weights, prev_biases) # update weights and biases using momentum\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(weights**2) + self.frobenius(biases**2) )\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization \n",
        "\n",
        "            theta = weights, biases\n",
        "        \n",
        "        total_error /= m # average total error across all mini-batches\n",
        "        return (theta, total_error) # return updated weights and biases and the total error\n",
        "    \n",
        "    def update_params_rmsprop(self, theta, d_theta, d_theta2, prev_weights2, prev_biases2, learning_rate, beta, eps):\n",
        "        weights, biases = theta\n",
        "        d_weights, d_biases = d_theta\n",
        "        d_weights2, d_biases2 = d_theta2\n",
        "\n",
        "        # Compute the exponential moving averages of squared gradients\n",
        "        prev_weights2 = beta * prev_weights2 + (1 - beta) * ((d_weights)**2)\n",
        "        prev_biases2 = beta * prev_biases2 + (1 - beta) * ((d_biases)**2)\n",
        "\n",
        "        # Compute the RMSProp update\n",
        "        W_ = learning_rate / ((prev_weights2)**0.5 + eps)\n",
        "        B_ = learning_rate / ((prev_biases2)**0.5 + eps)\n",
        "\n",
        "        # Update the parameters\n",
        "        # Apply weight decay to the weights\n",
        "        decay = (1 - self.weight_decay)\n",
        "        # Update weights and biases using the velocity and decayed weights\n",
        "        updated_weights = decay * weights - W_ * d_weights\n",
        "        updated_biases = decay * biases - B_ * d_biases\n",
        "\n",
        "        return ((np.array(updated_weights), np.array(updated_biases)), prev_weights2, prev_biases2)\n",
        "\n",
        "    def rmsprop(self, X, Y, theta, learning_rate, beta, eps, batch_size, L):\n",
        "        m = X.shape[1] # number of training examples\n",
        "        prev_weights2 = 0 # initialize previous weights to zero\n",
        "        prev_biases2 = 0 # initialize previous biases to zero\n",
        "        total_error = 0 # initialize total error to zero\n",
        "\n",
        "        # loop over mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            start = i\n",
        "            stop = i + batch_size\n",
        "            # compute output of the network\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) \n",
        "            W, B = theta\n",
        "            # compute L2 regularization term\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            # compute error\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            d_theta, d_theta2 = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) \n",
        "            \n",
        "            # update weights and biases using RMSProp                                                                    \n",
        "            theta, prev_weights, prev_biases = self.update_params_rmsprop(theta, d_theta, d_theta2, prev_weights2, prev_biases2, learning_rate, beta, eps)\n",
        "\n",
        "        # handle the last mini-batch if it is not a multiple of batch_size\n",
        "        if m % batch_size != 0:\n",
        "            start = m - m % batch_size\n",
        "            stop = m\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            d_theta, d_theta2= self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) # compute gradients\n",
        "        \n",
        "            theta, prev_weights2, prev_biases2 = self.update_params_rmsprop(theta, d_theta, d_theta2, prev_weights2, prev_biases2, learning_rate, beta, eps)\n",
        "            W, B = theta\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization \n",
        "\n",
        "        \n",
        "        total_error /= m # average total error across all mini-batches\n",
        "        return (theta, total_error) # return updated weights and biases and the total error\n",
        "  \n",
        "\n",
        "    def update_params_adam(self,theta,d_theta,d_theta2,prev_weights,prev_bias,prev_weights2,prev_biases2,learning_rate,beta1,beta2,eps,t):\n",
        "        weights, biases = theta\n",
        "        d_weights,d_biases = d_theta\n",
        "        d_weights2,d_biases2 = d_theta2\n",
        "        \n",
        "        # update the exponentially weighted averages of the gradients\n",
        "        prev_weights = beta1*prev_weights + (1-beta1)*d_weights\n",
        "        prev_bias = beta1*prev_bias + (1-beta1)*d_biases\n",
        "\n",
        "         # update the exponentially weighted averages of the squared gradients\n",
        "        prev_weights2=beta2*prev_weights2 + (1-beta2)*(d_weights2)\n",
        "        prev_biases2=beta2*prev_biases2 + (1-beta2)*(d_biases2)\n",
        "    \n",
        "        # bias correction to the weighted averages of the gradients\n",
        "        corr_m_w = prev_weights/(1-(beta1**t))\n",
        "        corr_m_b = prev_bias/(1-(beta1**t))\n",
        "\n",
        "        # bias correction to the weighted averages of the squared gradients\n",
        "        corr_v_w = prev_weights2/(1-(beta2**t))\n",
        "        corr_v_b = prev_biases2/(1-(beta2**t))\n",
        "\n",
        "        # calculate the update parameters using the bias-corrected averages of the gradients and squared gradients\n",
        "        corr_v_w = learning_rate/((corr_v_w)**0.5 + eps)\n",
        "        corr_v_b = learning_rate/((corr_v_b)**0.5 + eps)\n",
        "\n",
        "        # update the weights and biases using the update parameters and L2 regularization\n",
        "        weights = (1 - self.weight_decay)*weights - corr_v_w*corr_m_w\n",
        "        biases = (1 - self.weight_decay)*biases - corr_v_b*corr_m_b\n",
        "\n",
        "        theta = (np.array(weights),np.array(biases))\n",
        "        return (theta,prev_weights,prev_bias,prev_weights2,prev_biases2)\n",
        "\n",
        "    def adam(self,X,Y,theta,learning_rate,beta1,beta2,eps,batch_size,L):\n",
        "        m = X.shape[1] # number of training examples\n",
        "        prev_weights, prev_weights2 = 0,0 # initialize previous weights to zero\n",
        "        prev_biases, prev_biases2 = 0,0 # initialize previous biases to zero\n",
        "        total_error = 0 # initialize total error to zero\n",
        "\n",
        "        # loop over mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            start = i\n",
        "            stop = i + batch_size\n",
        "            # compute output of the network\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) \n",
        "            W, B = theta\n",
        "            # compute L2 regularization term\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            # compute error\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            d_theta, d_theta2 = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) \n",
        "            \n",
        "            # update weights and biases using Adam                                                                    \n",
        "            theta, prev_weights, prev_biases, prev_weights2, prev_biases2 = self.update_params_adam(theta, d_theta, d_theta2, prev_weights, prev_biases, prev_weights2, prev_biases2, learning_rate, beta1, beta2, eps, i+1)\n",
        "\n",
        "        # handle the last mini-batch if it is not a multiple of batch_size\n",
        "        if m % batch_size != 0:\n",
        "            start = m - m % batch_size\n",
        "            stop = m\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            d_theta, d_theta2= self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) # compute gradients\n",
        "        \n",
        "            theta, prev_weights, prev_biases, prev_weights2, prev_biases2 = self.update_params_adam(theta, d_theta, d_theta2, prev_weights, prev_biases, prev_weights2, prev_biases2, learning_rate, beta1, beta2, eps, i+1)\n",
        "            W, B = theta\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization \n",
        "\n",
        "        \n",
        "        total_error /= m # average total error across all mini-batches\n",
        "        return (theta, total_error) # return updated weights and biases and the total error\n",
        "  \n",
        "\n",
        "    def update_params_nadam(self,theta,d_theta,d_theta2,prev_weights,prev_bias,prev_weights2,prev_biases2,learning_rate,beta1,beta2,eps,t):\n",
        "        weights, biases = theta\n",
        "        d_weights,d_biases = d_theta\n",
        "        d_weights2,d_biases2 = d_theta2\n",
        "        \n",
        "        # update the exponentially weighted averages of the gradients\n",
        "        prev_weights = beta1*prev_weights + (1-beta1)*d_weights\n",
        "        prev_bias = beta1*prev_bias + (1-beta1)*d_biases\n",
        "\n",
        "         # update the exponentially weighted averages of the squared gradients\n",
        "        prev_weights2=beta2*prev_weights2 + (1-beta2)*(d_weights2)\n",
        "        prev_biases2=beta2*prev_biases2 + (1-beta2)*(d_biases2)\n",
        "    \n",
        "        beta_t = 1-(beta1**t)\n",
        "        beta2_t = 1-(beta2**t)\n",
        "        # bias correction to the weighted averages of the gradients\n",
        "        corr_m_w = beta1*prev_weights/beta_t + ((1-beta1)/beta_t)*d_weights\n",
        "        corr_m_b = beta1*prev_bias/beta_t + ((1-beta1)/beta_t)*d_biases\n",
        "\n",
        "        # bias correction to the weighted averages of the squared gradients\n",
        "        corr_v_w = prev_weights2/beta2_t\n",
        "        corr_v_b = prev_biases2/beta2_t\n",
        "\n",
        "        # calculate the update parameters using the bias-corrected averages of the gradients and squared gradients\n",
        "        corr_v_w = learning_rate/((corr_v_w)**0.5 + eps)\n",
        "        corr_v_b = learning_rate/((corr_v_b)**0.5 + eps)\n",
        "\n",
        "        # update the weights and biases using the update parameters and L2 regularization\n",
        "        weights = (1 - self.weight_decay)*weights - corr_v_w*corr_m_w\n",
        "        biases = (1 - self.weight_decay)*biases - corr_v_b*corr_m_b\n",
        "\n",
        "        theta = (np.array(weights),np.array(biases))\n",
        "        return (theta,prev_weights,prev_bias,prev_weights2,prev_biases2)\n",
        "\n",
        "    def nadam(self,X,Y,theta,learning_rate,beta1,beta2,eps,batch_size,L):\n",
        "        m = X.shape[1] # number of training examples\n",
        "        prev_weights, prev_weights2 = 0,0 # initialize previous weights to zero\n",
        "        prev_biases, prev_biases2 = 0,0 # initialize previous biases to zero\n",
        "        total_error = 0 # initialize total error to zero\n",
        "\n",
        "        weights, biases = theta\n",
        "        \n",
        "        # loop over mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            start = i\n",
        "            stop = i + batch_size\n",
        "            # compute output of the network\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) \n",
        "            W, B = theta\n",
        "            # compute L2 regularization term\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            # compute error\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            d_theta, d_theta2 = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) \n",
        "            \n",
        "            # update weights and biases using Adam                                                                    \n",
        "            theta, prev_weights, prev_biases, prev_weights2, prev_biases2 = self.update_params_nadam(theta, d_theta, d_theta2, prev_weights, prev_biases, prev_weights2, prev_biases2, learning_rate, beta1, beta2, eps, i+1)\n",
        "\n",
        "        # handle the last mini-batch if it is not a multiple of batch_size\n",
        "        if m % batch_size != 0:\n",
        "            start = m - m % batch_size\n",
        "            stop = m\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            d_theta, d_theta2= self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) # compute gradients\n",
        "        \n",
        "            theta, prev_weights, prev_biases, prev_weights2, prev_biases2 = self.update_params_nadam(theta, d_theta, d_theta2, prev_weights, prev_biases, prev_weights2, prev_biases2, learning_rate, beta1, beta2, eps, i+1)\n",
        "            W, B = theta\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization \n",
        "\n",
        "        \n",
        "        total_error /= m # average total error across all mini-batches\n",
        "        return (theta, total_error) # return updated weights and biases and the total error\n",
        "  \n",
        "    # Function to perform optimization based on the specified optimizer\n",
        "    def optimizations(self, theta, L):\n",
        "        # If optimizer is stochastic gradient descent\n",
        "        if self.optimizer == 'sgd':\n",
        "            # Perform mini-batch gradient descent on the training data\n",
        "            return self.sgd(X_train, Y_train, theta, self.learning_rate, 1, L)\n",
        "        elif self.optimizer == 'momentum':\n",
        "            return self.gd_momentum(X_train,Y_train,theta,self.learning_rate,self.batch_size,self.gamma,L)\n",
        "        elif self.optimizer == 'nesterov':\n",
        "            return self.gd_nesterov(X_train,Y_train,theta,self.learning_rate,self.batch_size,self.gamma,L)\n",
        "        elif self.optimizer == 'rmsprop':\n",
        "            return self.rmsprop(X_train,Y_train,theta,self.learning_rate,self.beta,self.eps,self.batch_size,L)\n",
        "        elif self.optimizer == 'adam':\n",
        "            return self.adam(X_train,Y_train,theta,self.learning_rate,self.beta1,self.beta2,self.eps,self.batch_size,L)\n",
        "        elif self.optimizer == 'nadam':\n",
        "            return self.nadam(X_train,Y_train,theta,self.learning_rate,self.beta1,self.beta2,self.eps,self.batch_size,L)\n",
        "      \n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        # perform optimization on the model's parameters (theta) and get train loss\n",
        "        self.theta, train_loss = self.optimizations(self.theta, self.L)\n",
        "\n",
        "        # make predictions on the training set\n",
        "        outputs_train = self.feedforward(X_train, self.theta, self.L)\n",
        "        Y_pred_train = np.argmax(outputs_train[1][-1], axis=0)\n",
        "        Y_true_train = np.argmax(Y_train, axis=0)\n",
        "\n",
        "        # calculate training accuracy\n",
        "        train_acc = accuracy_score(Y_true_train, Y_pred_train)\n",
        "\n",
        "        # make predictions on the validation set\n",
        "        outputs_val = self.feedforward(X_val, self.theta, self.L)\n",
        "\n",
        "        # calculate validation loss\n",
        "        val_loss = self.val_error(Y_val, outputs_val, self.loss_function)\n",
        "        Y_pred_val = np.argmax(outputs_val[1][-1], axis=0)\n",
        "        Y_true_val = np.argmax(Y_val, axis=0)\n",
        "\n",
        "        # calculate validation accuracy\n",
        "        val_acc = accuracy_score(Y_true_val, Y_pred_val)\n",
        "\n",
        "        # return training and validation accuracies and losses\n",
        "        return train_acc, train_loss, val_acc, val_loss\n",
        "    \n",
        "    # def fit_test(self):\n",
        "    #     # perform optimization on the model's parameters (theta) and get train loss\n",
        "    #     self.theta, train_loss = self.optimizations(self.theta, self.L)\n",
        "\n",
        "    #     # make predictions on the training set\n",
        "    #     outputs_train = self.feedforward(X_train, self.theta, self.L)\n",
        "    #     Y_pred_train = np.argmax(outputs_train[1][-1], axis=0)\n",
        "    #     Y_true_train = np.argmax(Y_train, axis=0)\n",
        "\n",
        "    #     # calculate training accuracy\n",
        "    #     train_acc = accuracy_score(Y_true_train, Y_pred_train)\n",
        "\n",
        "    #     # make predictions on the test set\n",
        "    #     outputs_val = self.feedforward(X_test, self.theta, self.L)\n",
        "\n",
        "    #     # calculate test loss\n",
        "    #     test_loss = self.val_error(Y_test, outputs_val, self.loss_function)\n",
        "    #     Y_pred_val = np.argmax(outputs_val[1][-1], axis=0)\n",
        "    #     Y_true_val = np.argmax(Y_test, axis=0)\n",
        "\n",
        "    #     # calculate test accuracy\n",
        "    #     test_acc = accuracy_score(Y_true_val, Y_pred_val)\n",
        "\n",
        "    #     # return training and test accuracies and losses\n",
        "    #     return train_acc, train_loss, test_acc, test_loss\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        # get the number of hidden layers\n",
        "        L = len(self.hidden_layers) + 1\n",
        "\n",
        "        # make predictions on the test set\n",
        "        outputs = self.feedforward(X_test, self.theta, L)\n",
        "        Y_pred = np.argmax(outputs[1][-1], axis=0)\n",
        "\n",
        "        # return predicted labels\n",
        "        return Y_pred"
      ],
      "metadata": {
        "id": "-NRNnwzfU43Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_models  = []"
      ],
      "metadata": {
        "id": "qbYSgTaCZTXO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def best_model(tuned_models):\n",
        "  sorted_list = sorted(tuned_models, key = lambda d : d['validation_accuracy'],reverse=True)\n",
        "  best_model = sorted_list[0][\"model\"]\n",
        "  return (sorted_list, best_model)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # with wandb.init() as run:\n",
        "    model = FeedForwardNN(config=None)\n",
        "    # run.name = model.run_name\n",
        "    train_acc,train_loss,val_acc,val_loss = 0,0,0,0\n",
        "    for epoch in range(20):\n",
        "        train_acc,train_loss,val_acc,val_loss = model.fit()  # model training code here\n",
        "        print({\n",
        "        \"epochs\": epoch,\n",
        "        \"accuracy\":train_acc,\n",
        "        \"loss\":train_loss,\n",
        "        \"validation_accuracy\": val_acc,\n",
        "        \"validation_loss\": val_loss,\n",
        "        })\n",
        "        # wandb.log(metrics) \n",
        "    # print(run.name)\n",
        "    tuned_models.append({\n",
        "        \"accuracy\":train_acc,\n",
        "        \"loss\":train_loss,\n",
        "        \"validation_accuracy\": val_acc,\n",
        "        \"validation_loss\": val_loss,\n",
        "        \"model\":model\n",
        "    })  \n",
        "\n",
        "\n",
        "\n",
        "train()  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9ibUE-XVZdH",
        "outputId": "c53dd44a-1201-42ed-bcf7-8d79b7e8dcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epochs': 0, 'accuracy': 0.8535185185185186, 'loss': 0.6628250572097459586, 'validation_accuracy': 0.8446666666666667, 'validation_loss': 0.43420532847324388792}\n",
            "{'epochs': 1, 'accuracy': 0.8757962962962963, 'loss': 0.40235168603769215316, 'validation_accuracy': 0.8636666666666667, 'validation_loss': 0.37643314530730460817}\n",
            "{'epochs': 2, 'accuracy': 0.8859444444444444, 'loss': 0.35092108303638460857, 'validation_accuracy': 0.8665, 'validation_loss': 0.36216592755619782343}\n",
            "{'epochs': 3, 'accuracy': 0.8935, 'loss': 0.32180806416134436463, 'validation_accuracy': 0.8705, 'validation_loss': 0.35697027751231945263}\n",
            "{'epochs': 4, 'accuracy': 0.900425925925926, 'loss': 0.29761846765460826697, 'validation_accuracy': 0.8746666666666667, 'validation_loss': 0.35043470261584015396}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(models_sorted, bestmodel) = best_model(tuned_models)"
      ],
      "metadata": {
        "id": "2skAdQMcbOyL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "b53fba77-6775-4777-ae21-29b55ea68649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6b340455c755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mmodels_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuned_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_true_test = np.argmax(Y_test,0)\n",
        "Y_true_pred = bestmodel.predict(X_test)"
      ],
      "metadata": {
        "id": "CLb1V0dyaJeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCutIrP5oHMa"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_test = confusion_matrix(Y_true_test, Y_true_pred, normalize='true')\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = sns.heatmap(bern_cnf_matrix_test, annot=bern_cnf_matrix_test,xticklabels=class_names, yticklabels=class_names)\n",
        "ax.set_title(\"Confusion Matrix (Test set)\", size=16)\n",
        "ax.set_xlabel(\"Predicted Class\", size=14)\n",
        "ax.set_ylabel(\"True Class\", size=14)\n",
        "plt.savefig(\"testmatrix_best\")\n",
        "img2 = plt.imread(\"testmatrix_best.png\")\n",
        "#wandb.init(project=\"CS6910 ASSIGNMENT 1\", entity=\"dlstack\", name=\"CONFUSION_MATRIX\")\n",
        "#wandb.log({\"Confusion Matrix - Test set 3\": wandb.sklearn.plot_confusion_matrix(Y_true_test, Y_true_pred, class_names)})\n",
        "wandb.init(project=\"Testing\", entity=\"dl_research\", name=\"Confusion Matrix\")\n",
        "wandb.log({\"Confusion Matrix Best\": wandb.Image(img2)})\n",
        "wandb.finish()\n",
        "# wandb.sklearn.plot_confusion_matrix(Y_true_test, Y_true_pred, class_names)"
      ]
    }
  ]
}