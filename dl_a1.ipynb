{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oikn2018/CS6910_assignment_1/blob/main/dl_a1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing required packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.datasets import fashion_mnist\n",
        "# import wandb\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) "
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-03-06T16:06:51.244959Z",
          "iopub.execute_input": "2023-03-06T16:06:51.245375Z",
          "iopub.status.idle": "2023-03-06T16:06:51.251764Z",
          "shell.execute_reply.started": "2023-03-06T16:06:51.245338Z",
          "shell.execute_reply": "2023-03-06T16:06:51.250465Z"
        },
        "trusted": true,
        "id": "RloJUTVzCIqE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fashion_mnist dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
        "\n",
        "print(f\"Training data shape: {x_train.shape}, Training label shape: {y_train.shape}\")\n",
        "print(f\"Validation data shape: {x_val.shape}, Validation label shape: {y_val.shape}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-06T16:06:51.839789Z",
          "iopub.execute_input": "2023-03-06T16:06:51.840207Z",
          "iopub.status.idle": "2023-03-06T16:06:52.338048Z",
          "shell.execute_reply.started": "2023-03-06T16:06:51.840171Z",
          "shell.execute_reply": "2023-03-06T16:06:52.336816Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTdCpczrCIqK",
        "outputId": "321ebd92-3b02-4318-f53a-b178eed6ba29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Training data shape: (54000, 28, 28), Training label shape: (54000,)\n",
            "Validation data shape: (6000, 28, 28), Validation label shape: (6000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the input data for training, validation, and testing sets\n",
        "X_train = np.reshape(x_train, (x_train.shape[0], -1)).T\n",
        "X_val = np.reshape(x_val, (x_val.shape[0], -1)).T\n",
        "X_test = np.reshape(x_test, (x_test.shape[0], -1)).T\n",
        "\n",
        "# Normalize the input data to have values between 0 and 1\n",
        "X_train = X_train / 255.\n",
        "X_val = X_val / 255.\n",
        "X_test = X_test / 255.\n",
        "\n",
        "# Convert the target labels into one-hot encoded vectors\n",
        "Y_train = np.eye(np.max(y_train) + 1)[y_train].T\n",
        "Y_val = np.eye(np.max(y_val) + 1)[y_val].T\n",
        "Y_test = np.eye(np.max(y_test) + 1)[y_test].T\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}, Training label shape: {Y_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}, Validation label shape: {Y_val.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}, Testing label shape: {Y_test.shape}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-06T16:06:52.375021Z",
          "iopub.execute_input": "2023-03-06T16:06:52.375398Z",
          "iopub.status.idle": "2023-03-06T16:06:52.534799Z",
          "shell.execute_reply.started": "2023-03-06T16:06:52.375364Z",
          "shell.execute_reply": "2023-03-06T16:06:52.533638Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2tDeIXYCIqQ",
        "outputId": "b806b41d-ec06-4042-b520-8115eb19ab6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (784, 54000), Training label shape: (10, 54000)\n",
            "Validation data shape: (784, 6000), Validation label shape: (10, 6000)\n",
            "Testing data shape: (784, 10000), Testing label shape: (10, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNN:\n",
        "    def __init__(self,config=None,epochs=5,hidden_layers=[64,64,64],weight_decay=0,learning_rate=1e-3,optimizer='rmsprop',batch_size=16,weight_initialization='random',activations='sigmoid',loss_function='cross-entropy',output_function='softmax',gamma=0.9,beta=0.9,beta1=0.9,beta2=0.999,eps=1e-8):\n",
        "        # Constructor that initializes the neural network\n",
        "        if config is not None:\n",
        "            # If a config dictionary is passed, use its values to initialize the parameters\n",
        "            self.epochs = config[\"epochs\"]\n",
        "            self.learning_rate = config[\"learning_rate\"]\n",
        "            self.weight_decay = config[\"weight_decay\"]\n",
        "            self.optimizer = config[\"optimizer\"]\n",
        "            self.batch_size = config[\"batch_size\"]\n",
        "            self.weight_initialization = config[\"weight_initialization\"]\n",
        "            self.activations = config[\"activations\"]\n",
        "            self.hidden_layers = [config[\"hidden_layers_size\"] for x in range(config[\"no_hidden_layers\"])]\n",
        "        else:\n",
        "            # If no config dictionary is passed, use the default values to initialize the parameters\n",
        "            self.epochs = epochs\n",
        "            self.learning_rate = learning_rate\n",
        "            self.weight_decay = weight_decay\n",
        "            self.optimizer = optimizer\n",
        "            self.batch_size = batch_size\n",
        "            self.weight_initialization = weight_initialization\n",
        "            self.activations = activations\n",
        "            self.hidden_layers = hidden_layers\n",
        "\n",
        "        # Set the remaining parameters for the neural network\n",
        "        self.loss_function = loss_function\n",
        "        self.output_function = output_function\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "\n",
        "        # Initialize the neural network\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        # Set the number of neurons in each layer of the neural network\n",
        "        layers = self.hidden_layers + [Y_train.shape[0]]\n",
        "\n",
        "        # Initialize the weights and biases for each layer of the neural network\n",
        "        self.theta = self.initialize_parameters(X_train.shape[0],layers,self.weight_initialization)\n",
        "\n",
        "        # Calculate the regularization parameter\n",
        "        self.lambd = self.weight_decay/self.learning_rate\n",
        "\n",
        "        # Set the number of layers in the neural network\n",
        "        self.L = len(layers)\n",
        "\n",
        "\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "    \n",
        "    def tanh(self,x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def ReLu(self,x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def d_sigmoid(self,x):\n",
        "        return self.sigmoid(x)*(1.0 - self.sigmoid(x))\n",
        "    \n",
        "    def d_tanh(self,x):\n",
        "        return 1.0 - self.tanh(x)*self.tanh(x) \n",
        "\n",
        "    def d_ReLu(self,x):\n",
        "        return x > 0\n",
        "\n",
        "    def activation(self,x,n='sigmoid'):\n",
        "        if n == 'sigmoid':\n",
        "              return self.sigmoid(x)\n",
        "        elif n == 'tanh':\n",
        "              return self.tanh(x)\n",
        "        elif n == 'ReLu':\n",
        "              return self.ReLu(x)\n",
        "\n",
        "    def d_activation(self,x,n='sigmoid'):\n",
        "        if n == 'sigmoid':\n",
        "              return self.d_sigmoid(x)\n",
        "        elif n == 'tanh':\n",
        "              return self.d_tanh(x)\n",
        "        elif n=='ReLu':\n",
        "              return self.d_ReLu(x)\n",
        "\n",
        "\n",
        "    def softmax(self,x):\n",
        "        e = np.exp(x)\n",
        "        return e / np.sum(e,axis=0)\n",
        "\n",
        "    def output(self, x, output_function='softmax'):\n",
        "        if output_function == 'softmax':\n",
        "            return self.softmax(x)\n",
        "\n",
        "    def cross_entropy_error(self, Y, inputs):\n",
        "        Y_hat = inputs[1][-1]\n",
        "        return -1 * np.sum(Y * (np.log(Y_hat)))\n",
        "    \n",
        "    def squared_error(self, Y, inputs):\n",
        "        Y_hat = inputs[1][-1]\n",
        "        return (1 / 2) * np.sum((Y_hat - Y) ** 2)\n",
        "\n",
        "    def squared_error_val(self, Y, Y_hat, W, B):\n",
        "        Y_hat = inputs[1][-1]\n",
        "        W, B = self.theta\n",
        "        m = Y.shape[1]\n",
        "        return (1 / (2 * m)) * np.sum((Y_hat - Y) ** 2) + \\\n",
        "               (self.lambd / (2 * m)) * (self.frobenius(W ** 2) + self.frobenius(B ** 2))\n",
        "\n",
        "    def error(self, Y, inputs, loss_function='cross-entropy'):\n",
        "        if loss_function == 'cross-entropy':\n",
        "            return self.cross_entropy_error(Y, inputs)\n",
        "        elif loss_function == 'squared-error':\n",
        "            return self.squared_error(Y, inputs) \n",
        "\n",
        "    def cross_entropy_error_val(self, Y, inputs):\n",
        "        W, B = self.theta\n",
        "        Y_hat = inputs[1][-1]\n",
        "        m = Y.shape[1]\n",
        "        return (-1/m) * np.sum(Y * (np.log(Y_hat))) + (self.lambd/(2*m)) * (self.frobenius(W ** 2) + self.frobenius(B ** 2))\n",
        "\n",
        "    def val_error(self, Y, inputs, loss_function='cross-entropy'):\n",
        "        if loss_function == 'cross-entropy':\n",
        "            return self.cross_entropy_error_val(Y, inputs)\n",
        "        if loss_function == 'squared-error':\n",
        "            return self.squared_error_val(Y, inputs)\n",
        "\n",
        "#     def frobenius(self, M):\n",
        "#         return np.sqrt(np.sum(np.square(M)))\n",
        "\n",
        "    def random_initialize_parameters(self, n, layers):\n",
        "        L = len(layers)\n",
        "        biases = [np.float128(np.zeros((layers[i], 1))) for i in range(L)]\n",
        "        weights = [np.float128(np.random.randn(layers[i], n) if i == 0 else np.random.randn(layers[i], layers[i - 1])) for i in range(L)]\n",
        "        return (np.array(weights), np.array(biases))\n",
        "\n",
        "\n",
        "    def initialize_parameters(self,n,layers,t):\n",
        "        if t == 'random':\n",
        "              return self.random_initialize_parameters(n,layers)\n",
        "        #     elif t == 'Xavier':\n",
        "        #       return self.Xavier_initialize_parameters(n,layers)\n",
        "\n",
        "    def frobenius(self,X):\n",
        "        s=0\n",
        "        for x in X:\n",
        "          s += np.sum(x)\n",
        "        return s\n",
        "#     def frobenius(self, X):\n",
        "#         return np.linalg.norm(X, ord='fro')\n",
        "\n",
        "    def feedforward(self,X,theta,L):\n",
        "        H = X\n",
        "        weights ,biases = theta\n",
        "        activations = []\n",
        "        pre_activations = []\n",
        "        for k in range(L-1):\n",
        "              A = biases[k] + (weights[k] @ H)\n",
        "              H = self.activation(A,self.activations)\n",
        "              pre_activations.append(A)\n",
        "              activations.append(H)\n",
        "        \n",
        "        AL = biases[L-1] + (weights[L-1] @ H)\n",
        "        Y_hat = self.output(AL,self.output_function)\n",
        "        pre_activations.append(AL)\n",
        "        activations.append(Y_hat)\n",
        "        return (np.array(pre_activations),np.array(activations))\n",
        "\n",
        "    def backprop(self,X,Y,inputs,theta,batch_size,L):\n",
        "        # Initialize empty lists for storing gradients\n",
        "        d_biases, d_weights = [], []\n",
        "        d_biases2 = []\n",
        "        d_weights2 = []\n",
        "        # Extract pre-activations and activations from the inputs\n",
        "        pre_activations, activations = inputs\n",
        "        # Get the predicted output\n",
        "        Y_hat = activations[-1]\n",
        "#         # Retrieve the weights and biases from the current model parameters\n",
        "#         weights, biases = theta\n",
        "\n",
        "        if self.loss_function == 'squared-error':\n",
        "          d_AL = Y_hat*(Y_hat - Y)*(1 - Y_hat)\n",
        "        elif self.loss_function == 'cross-entropy':\n",
        "          d_AL = Y_hat - Y\n",
        "        # Loop over the layers in reverse order to calculate the gradients\n",
        "        for k in range(L-1, -1, -1):\n",
        "            # Calculate the gradients for the weights and biases\n",
        "            d_W = (1/batch_size)*(d_AL @ activations[k-1].T) if k > 0 else (1/batch_size)*(d_AL @ X.T)\n",
        "            d_W2 = (1 / batch_size) * (d_AL ** 2 @ (activations[k-1].T) ** 2) if k>0 else (1 / batch_size) * (d_AL ** 2 @ (X.T) ** 2)\n",
        "            d_B = (1/batch_size)*np.sum(d_AL, axis=1, keepdims=True)\n",
        "            d_B2 = (1 / batch_size) * np.sum(d_AL ** 2, axis=1, keepdims=True)\n",
        "\n",
        "            # Calculate the derivative of the activation function and backpropagate the error to the previous layer\n",
        "            if k > 0:\n",
        "                d_AL = (theta[0][k].T @ d_AL) * self.d_activation(pre_activations[k-1], self.activations)\n",
        "            # Add the gradients to the lists\n",
        "            d_weights.insert(0, d_W)\n",
        "            d_biases.insert(0, d_B)\n",
        "            d_weights2.insert(0, d_W2)\n",
        "            d_biases2.insert(0, d_B2)\n",
        "        d_theta = (np.array(d_weights),np.array(d_biases))\n",
        "        d_theta2 = (np.array(d_weights2), np.array(d_biases2))\n",
        "        \n",
        "        return (d_theta, d_theta2)\n",
        "\n",
        "    # Function to update weights and biases based on the calculated gradients and learning rate\n",
        "    def update_params(self, theta, d_theta, learning_rate):\n",
        "        weights, biases = theta\n",
        "        d_weights, d_biases = d_theta\n",
        "        updated_weights = (1 - self.weight_decay)*weights - learning_rate*d_weights\n",
        "        updated_biases = (1 - self.weight_decay)*biases - learning_rate*d_biases\n",
        "        return updated_weights, updated_biases\n",
        "\n",
        "    # Function to perform mini-batch gradient descent on the given data\n",
        "    def sgd(self, X, Y, theta, learning_rate, batch_size, L):\n",
        "        m = X.shape[1]\n",
        "        total_error = 0\n",
        "        # loop over mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            start = i\n",
        "            stop = i + batch_size\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            W, B = theta\n",
        "            # compute L2 regularization term\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            # compute error\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization\n",
        "            # compute gradients\n",
        "            d_theta, _ = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) \n",
        "            # update weights and biases                                                                    \n",
        "            theta = self.update_params(theta, d_theta, learning_rate)\n",
        "        # handle the last mini-batch if it is not a multiple of batch_size\n",
        "        if m % batch_size != 0:\n",
        "            start = m - m % batch_size\n",
        "            stop = m\n",
        "#             start = i*batch_size\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            d_theta, _ = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, m % batch_size, L) # compute gradients\n",
        "            theta = self.update_params(theta, d_theta, learning_rate) # update weights and biases\n",
        "            W, B = theta\n",
        "\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization \n",
        "            \n",
        "        # Calculate the average error\n",
        "        avg_err = total_error/m\n",
        "        # Return the updated theta and average error\n",
        "        return (theta, avg_err)\n",
        "    \n",
        "    def update_params_momentum(self, theta, d_theta, learning_rate, gamma, prev_weight, prev_bias):\n",
        "        weights, biases = theta\n",
        "        d_weights,d_biases = d_theta\n",
        "        \n",
        "        # Calculate the velocity for weights and biases\n",
        "        v_weight = gamma * prev_weight + learning_rate * d_weights\n",
        "        v_bias = gamma * prev_bias + learning_rate * d_biases\n",
        "\n",
        "        # Apply weight decay to the weights\n",
        "        decay = (1 - self.weight_decay)\n",
        "\n",
        "        # Update weights and biases using the velocity and decayed weights\n",
        "        updated_weights = decay*weights - v_weight\n",
        "        updated_biases = decay*biases - v_bias\n",
        "\n",
        "        return (updated_weights, updated_biases, v_weight, v_bias)\n",
        "\n",
        "    def gd_momentum(self, X, Y, theta, learning_rate, batch_size, gamma, L):\n",
        "        m = X.shape[1] # number of training examples\n",
        "        prev_weights = 0 # initialize previous weights to zero\n",
        "        prev_biases = 0 # initialize previous biases to zero\n",
        "        total_error = 0 # initialize total error to zero\n",
        "\n",
        "        # loop over mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            start = i\n",
        "            stop = i + batch_size\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            W, B = theta\n",
        "            # compute L2 regularization term\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            # compute error\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization\n",
        "            # compute gradients\n",
        "            d_theta, _ = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) \n",
        "            # update weights and biases using momentum                                                                     \n",
        "            weights, biases, prev_weights, prev_biases = self.update_params_momentum(theta, d_theta, learning_rate, gamma, prev_weights, prev_biases)\n",
        "            theta = weights, biases\n",
        "        # handle the last mini-batch if it is not a multiple of batch_size\n",
        "        if m % batch_size != 0:\n",
        "            start = m - m % batch_size\n",
        "            stop = m\n",
        "#             start = i*batch_size\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            d_theta = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, m % batch_size, L) # compute gradients\n",
        "        \n",
        "            weights, biases, prev_weights, prev_biases = self.update_params_momentum(theta, d_theta, learning_rate, gamma, prev_weights, prev_biases) # update weights and biases using momentum\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(weights**2) + self.frobenius(biases**2) )\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization \n",
        "            \n",
        "            \n",
        "\n",
        "            theta = weights, biases\n",
        "        \n",
        "        total_error /= m # average total error across all mini-batches\n",
        "        return (theta, total_error) # return updated weights and biases and the total error\n",
        "\n",
        "    def update_params_nesterov(self, theta, d_theta, learning_rate, gamma, prev_weight, prev_bias):\n",
        "        weights, biases = theta\n",
        "        d_weights,d_biases = d_theta\n",
        "        \n",
        "        # Calculate the velocity for weights and biases\n",
        "        v_weight = gamma * prev_weight + learning_rate * d_weights\n",
        "        v_bias = gamma * prev_bias + learning_rate * d_biases\n",
        "\n",
        "        # Apply weight decay to the weights\n",
        "        decay = (1 - self.weight_decay)\n",
        "\n",
        "        # Update weights and biases using the velocity and decayed weights\n",
        "        updated_weights = decay*weights - v_weight\n",
        "        updated_biases = decay*biases - v_bias\n",
        "\n",
        "        return (updated_weights, updated_biases, v_weight, v_bias)\n",
        "\n",
        "    def gd_nesterov(self, X, Y, theta, learning_rate, batch_size, gamma, L):\n",
        "        m = X.shape[1] # number of training examples\n",
        "        prev_weights = 0 # initialize previous weights to zero\n",
        "        prev_biases = 0 # initialize previous biases to zero\n",
        "        total_error = 0 # initialize total error to zero\n",
        "\n",
        "        weights, biases = theta\n",
        "        # loop over mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            start = i\n",
        "            stop = i + batch_size\n",
        "            # compute output of the network\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) \n",
        "            W, B = theta\n",
        "            # compute L2 regularization term\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            # compute error\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            v_weight=gamma*prev_weights\n",
        "            v_biases=gamma*prev_biases\n",
        "            theta2=weights-v_weight,biases-v_biases\n",
        "            d_theta, _ = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta2, batch_size, L) \n",
        "            \n",
        "            # update weights and biases using momentum                                                                     \n",
        "            weights, biases, prev_weights, prev_biases = self.update_params_nesterov(theta, d_theta, learning_rate, gamma, prev_weights, prev_biases)\n",
        "            theta = weights, biases\n",
        "        # handle the last mini-batch if it is not a multiple of batch_size\n",
        "        if m % batch_size != 0:\n",
        "            start = m - m % batch_size\n",
        "            stop = m\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            v_weight=gamma*prev_weights\n",
        "            v_biases=gamma*prev_biases\n",
        "            theta2=weights-v_weight,biases-v_biases\n",
        "            d_theta, _= self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta2, m % batch_size, L) # compute gradients\n",
        "        \n",
        "            weights, biases, prev_weights, prev_biases = self.update_params_nesterov(theta, d_theta, learning_rate, gamma, prev_weights, prev_biases) # update weights and biases using momentum\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(weights**2) + self.frobenius(biases**2) )\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization \n",
        "\n",
        "            theta = weights, biases\n",
        "        \n",
        "        total_error /= m # average total error across all mini-batches\n",
        "        return (theta, total_error) # return updated weights and biases and the total error\n",
        "    \n",
        "    def update_params_rmsprop(self, theta, d_theta, d_theta2, prev_weights2, prev_biases2, learning_rate, beta, eps):\n",
        "        weights, biases = theta\n",
        "        d_weights, d_biases = d_theta\n",
        "        d_weights2, d_biases2 = d_theta2\n",
        "\n",
        "        # Compute the exponential moving averages of squared gradients\n",
        "        prev_weights2 = beta * prev_weights2 + (1 - beta) * (d_weights)**2\n",
        "        prev_biases2 = beta * prev_biases2 + (1 - beta) * (d_biases)**2\n",
        "\n",
        "        # Compute the RMSProp update\n",
        "        W_ = learning_rate / ((prev_weights2 + eps)**0.5)\n",
        "        B_ = learning_rate / ((prev_biases2 + eps)**0.5)\n",
        "\n",
        "        # Update the parameters\n",
        "        # Apply weight decay to the weights\n",
        "        decay = (1 - self.weight_decay)\n",
        "        # Update weights and biases using the velocity and decayed weights\n",
        "        updated_weights = decay * weights - W_ * d_weights\n",
        "        updated_biases = decay * biases - B_ * d_biases\n",
        "\n",
        "        return ((np.array(updated_weights), np.array(updated_biases)), prev_weights2, prev_biases2)\n",
        "\n",
        "    def rmsprop(self, X, Y, theta, learning_rate, beta, eps, batch_size, L):\n",
        "        m = X.shape[1] # number of training examples\n",
        "        prev_weights2 = 0 # initialize previous weights to zero\n",
        "        prev_biases2 = 0 # initialize previous biases to zero\n",
        "        total_error = 0 # initialize total error to zero\n",
        "\n",
        "        # loop over mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            start = i\n",
        "            stop = i + batch_size\n",
        "            # compute output of the network\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) \n",
        "            W, B = theta\n",
        "            # compute L2 regularization term\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            # compute error\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            d_theta, d_theta2 = self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) \n",
        "            \n",
        "            # update weights and biases using RMSProp                                                                    \n",
        "            theta, prev_weights, prev_biases = self.update_params_rmsprop(theta, d_theta, d_theta2, prev_weights2, prev_biases2, learning_rate, beta, eps)\n",
        "\n",
        "        # handle the last mini-batch if it is not a multiple of batch_size\n",
        "        if m % batch_size != 0:\n",
        "            start = m - m % batch_size\n",
        "            stop = m\n",
        "            inputs = self.feedforward(X[:, start:stop], theta, L) # compute output of the network\n",
        "            \n",
        "            # Compute gradients using backpropagation\n",
        "            d_theta, d_theta2= self.backprop(X[:, start:stop], Y[:, start:stop], inputs, theta, batch_size, L) # compute gradients\n",
        "        \n",
        "            theta, prev_weights2, prev_biases2 = self.update_params_rmsprop(theta, d_theta, d_theta2, prev_weights2, prev_biases2, learning_rate, beta, eps)\n",
        "            W, B = theta\n",
        "            regularization = (self.lambd / 2) * (self.frobenius(W**2) + self.frobenius(B**2) )\n",
        "            total_error += self.error(Y[:, start:stop], inputs, self.loss_function) + regularization \n",
        "\n",
        "        \n",
        "        total_error /= m # average total error across all mini-batches\n",
        "        return (theta, total_error) # return updated weights and biases and the total error\n",
        "    \n",
        "    # Function to perform optimization based on the specified optimizer\n",
        "    def optimizations(self, theta, L):\n",
        "        # If optimizer is stochastic gradient descent\n",
        "        if self.optimizer == 'sgd':\n",
        "            # Perform mini-batch gradient descent on the training data\n",
        "            return self.sgd(X_train, Y_train, theta, self.learning_rate, 1, L)\n",
        "        elif self.optimizer == 'momentum':\n",
        "            return self.gd_momentum(X_train,Y_train,theta,self.learning_rate,self.batch_size,self.gamma,L)\n",
        "        elif self.optimizer == 'nesterov':\n",
        "            return self.gd_nesterov(X_train,Y_train,theta,self.learning_rate,self.batch_size,self.gamma,L)\n",
        "        elif self.optimizer == 'rmsprop':\n",
        "            return self.rmsprop(X_train,Y_train,theta,self.learning_rate,self.beta,self.eps,self.batch_size,L)\n",
        "        elif self.optimizer == 'adam':\n",
        "            return self.adam(X_train,Y_train,theta,self.learning_rate,self.beta1,self.beta2,self.eps,self.batch_size,L)\n",
        "        elif self.optimizer == 'nadam':\n",
        "            return self.nadam(X_train,Y_train,theta,self.learning_rate,self.beta1,self.beta2,self.eps,self.batch_size,L)\n",
        "      \n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        # perform optimization on the model's parameters (theta) and get train loss\n",
        "        self.theta, train_loss = self.optimizations(self.theta, self.L)\n",
        "\n",
        "        # make predictions on the training set\n",
        "        outputs_train = self.feedforward(X_train, self.theta, self.L)\n",
        "        Y_pred_train = np.argmax(outputs_train[1][-1], axis=0)\n",
        "        Y_true_train = np.argmax(Y_train, axis=0)\n",
        "\n",
        "        # calculate training accuracy\n",
        "        train_acc = accuracy_score(Y_true_train, Y_pred_train)\n",
        "\n",
        "        # make predictions on the validation set\n",
        "        outputs_val = self.feedforward(X_val, self.theta, self.L)\n",
        "\n",
        "        # calculate validation loss\n",
        "        val_loss = self.val_error(Y_val, outputs_val, self.loss_function)\n",
        "        Y_pred_val = np.argmax(outputs_val[1][-1], axis=0)\n",
        "        Y_true_val = np.argmax(Y_val, axis=0)\n",
        "\n",
        "        # calculate validation accuracy\n",
        "        val_acc = accuracy_score(Y_true_val, Y_pred_val)\n",
        "\n",
        "        # return training and validation accuracies and losses\n",
        "        return train_acc, train_loss, val_acc, val_loss\n",
        "    \n",
        "    \n",
        "    def predict(self, X_test):\n",
        "        # get the number of hidden layers\n",
        "        L = len(self.hidden_layers) + 1\n",
        "\n",
        "        # make predictions on the test set\n",
        "        outputs = self.feedforward(X_test, self.theta, L)\n",
        "        Y_pred = np.argmax(outputs[1][-1], axis=0)\n",
        "\n",
        "        # return predicted labels\n",
        "        return Y_pred"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-06T16:06:53.011769Z",
          "iopub.execute_input": "2023-03-06T16:06:53.012161Z",
          "iopub.status.idle": "2023-03-06T16:06:53.065516Z",
          "shell.execute_reply.started": "2023-03-06T16:06:53.012127Z",
          "shell.execute_reply": "2023-03-06T16:06:53.064343Z"
        },
        "trusted": true,
        "id": "g_JXeQMzCIqS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Models = []\n",
        "def train():\n",
        "        model = FeedForwardNN()\n",
        "        train_acc,train_loss,val_acc,val_loss = 0,0,0,0\n",
        "        for epoch in range(2):\n",
        "            '''config[\"epochs\"]'''\n",
        "            train_acc,train_loss,val_acc,val_loss = model.fit()  # model training code here\n",
        "            metrics = {\n",
        "            \"accuracy\":train_acc,\n",
        "             \"loss\":train_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "            \"validation_loss\": val_loss,\n",
        "             \"epochs\":epoch\n",
        "             }\n",
        "\n",
        "            print(metrics)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-06T16:06:53.590343Z",
          "iopub.execute_input": "2023-03-06T16:06:53.591267Z",
          "iopub.status.idle": "2023-03-06T16:06:53.598214Z",
          "shell.execute_reply.started": "2023-03-06T16:06:53.591213Z",
          "shell.execute_reply": "2023-03-06T16:06:53.596920Z"
        },
        "trusted": true,
        "id": "c8loy-vrCIqZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-06T16:17:16.259253Z",
          "iopub.execute_input": "2023-03-06T16:17:16.259720Z",
          "iopub.status.idle": "2023-03-06T16:24:29.027103Z",
          "shell.execute_reply.started": "2023-03-06T16:17:16.259678Z",
          "shell.execute_reply": "2023-03-06T16:24:29.025799Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtnVp2Z7CIqb",
        "outputId": "f3f959ca-8a83-4f94-b6e0-92a3e921bd56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.6487222222222222, 'loss': 1.3542035862490576869, 'validation_accuracy': 0.643, 'validation_loss': 0.9988250671212119614, 'epochs': 0}\n",
            "{'accuracy': 0.6997592592592593, 'loss': 0.9118069564959600619, 'validation_accuracy': 0.6911666666666667, 'validation_loss': 0.85069104264872425863, 'epochs': 1}\n",
            "{'accuracy': 0.7245185185185186, 'loss': 0.8069960552769965759, 'validation_accuracy': 0.716, 'validation_loss': 0.78002481335987113866, 'epochs': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "htjLTYNXCIqd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}